{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"(Lab 2) pix2pix.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyMJkio82pnluFoY+p/lbH69"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"szfSTP2MkCZs","colab_type":"text"},"source":["# Image-to-Image Translation : *pix2pix*"]},{"cell_type":"markdown","metadata":{"id":"DDiPyxsT-_ri","colab_type":"text"},"source":["## Download scripts & datasets"]},{"cell_type":"code","metadata":{"id":"DyhwLuqZQAhk","colab_type":"code","colab":{}},"source":["# download facades dataset\n","!mkdir ./dataset\n","!wget -N http://efrosgans.eecs.berkeley.edu/pix2pix/datasets/facades.tar.gz -O ./datasets/facades.tar.gz\n","!mkdir -p ./datasets/facades/\n","!tar -zxvf ./datasets/facades.tar.gz -C ./datasets/\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dJjSXzlAmCOI","colab_type":"text"},"source":["### Prepare DataLoader for MNIST dataset"]},{"cell_type":"code","metadata":{"id":"i5Fuxt5dP7IZ","colab_type":"code","colab":{}},"source":["import torch\n","from torch.utils.data import DataLoader\n","from torchvision import transforms\n","\n","# fix manual seed.\n","torch.manual_seed(1234)\n","\n","# set batch size.\n","BATCH_SIZE = 8\n","NUM_EXAMPLES = 8"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0XsjfNBzc_id","colab_type":"code","colab":{}},"source":["import os\n","from torch.utils.data import Dataset\n","import PIL\n","\n","# Custom Dataset \n","class CustomFacades(Dataset):\n","    def __init__(self, root, train=True, transform=None):\n","        self.root = root\n","        self.transform = transform       \n","        self.train = train\n","        self.dir = 'train' if train else 'test'\n","        self.len_train = 400\n","        self.len_test = 106\n","\n","    def __len__(self):\n","        if self.train:\n","            return self.len_train\n","        else:\n","            return self.len_test\n","\n","    def __getitem__(self, idx):\n","        data_idx = idx if self.train else idx + self.len_train\n","        X = PIL.Image.open(os.path.join(self.root, self.dir, '{}.jpg'.format(idx+1)))\n","        if self.transform is not None:\n","            X = self.transform(X)\n","        return X[..., 256:], X[..., :256]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uanjhhI5dIeH","colab_type":"code","colab":{}},"source":["# prepare dataloader.\n","tf = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))\n","    ])\n","\n","# train loader for training GAN\n","train_dataset = CustomFacades(root='./datasets/facades', train=True, \n","        transform=tf)\n","train_loader = DataLoader(dataset=train_dataset, num_workers=8, batch_size=BATCH_SIZE, shuffle=True)\n","\n","# test loader for examining test samples\n","test_dataset = CustomFacades(root='./datasets/facades', train=False, transform=tf)\n","test_loader = DataLoader(dataset=test_dataset, num_workers=8, batch_size=BATCH_SIZE, shuffle=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yf6CYjavmK_y","colab_type":"text"},"source":["### Define GANs Models"]},{"cell_type":"markdown","metadata":{"id":"GFVC5C4wmQ5k","colab_type":"text"},"source":["#### Define Generator"]},{"cell_type":"code","metadata":{"id":"iuiBN6NTFDgN","colab_type":"code","colab":{}},"source":["import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class Generator(nn.Module):\n","    def __init__(self):\n","        super(Generator, self).__init__()\n","        # borrowed from : https://github.com/znxlwm/pytorch-pix2pix/blob/master/network.py\n","        d = 64\n","        # Unet Encoder\n","        self.conv1 = nn.Conv2d(3, d, 4, 2, 1)\n","        self.conv2 = nn.Conv2d(d, d * 2, 4, 2, 1)\n","        self.conv2_bn = nn.BatchNorm2d(d * 2)\n","        self.conv3 = nn.Conv2d(d * 2, d * 4, 4, 2, 1)\n","        self.conv3_bn = nn.BatchNorm2d(d * 4)\n","        self.conv4 = nn.Conv2d(d * 4, d * 8, 4, 2, 1)\n","        self.conv4_bn = nn.BatchNorm2d(d * 8)\n","        self.conv5 = nn.Conv2d(d * 8, d * 8, 4, 2, 1)\n","        self.conv5_bn = nn.BatchNorm2d(d * 8)\n","        self.conv6 = nn.Conv2d(d * 8, d * 8, 4, 2, 1)\n","        self.conv6_bn = nn.BatchNorm2d(d * 8)\n","        self.conv7 = nn.Conv2d(d * 8, d * 8, 4, 2, 1)\n","        self.conv7_bn = nn.BatchNorm2d(d * 8)\n","        self.conv8 = nn.Conv2d(d * 8, d * 8, 4, 2, 1)\n","\n","        # Unet Decoder\n","        self.deconv1 = nn.ConvTranspose2d(d * 8, d * 8, 4, 2, 1)\n","        self.deconv1_bn = nn.BatchNorm2d(d * 8)\n","        self.deconv2 = nn.ConvTranspose2d(d * 8 * 2, d * 8, 4, 2, 1)\n","        self.deconv2_bn = nn.BatchNorm2d(d * 8)\n","        self.deconv3 = nn.ConvTranspose2d(d * 8 * 2, d * 8, 4, 2, 1)\n","        self.deconv3_bn = nn.BatchNorm2d(d * 8)\n","        self.deconv4 = nn.ConvTranspose2d(d * 8 * 2, d * 8, 4, 2, 1)\n","        self.deconv4_bn = nn.BatchNorm2d(d * 8)\n","        self.deconv5 = nn.ConvTranspose2d(d * 8 * 2, d * 4, 4, 2, 1)\n","        self.deconv5_bn = nn.BatchNorm2d(d * 4)\n","        self.deconv6 = nn.ConvTranspose2d(d * 4 * 2, d * 2, 4, 2, 1)\n","        self.deconv6_bn = nn.BatchNorm2d(d * 2)\n","        self.deconv7 = nn.ConvTranspose2d(d * 2 * 2, d, 4, 2, 1)\n","        self.deconv7_bn = nn.BatchNorm2d(d)\n","        self.deconv8 = nn.ConvTranspose2d(d * 2, 3, 4, 2, 1)\n","        \n","    def forward(self, x):\n","        # encoding\n","        e1 = self.conv1(x)\n","        e2 = self.conv2_bn(self.conv2(F.leaky_relu(e1, 0.2)))\n","        e3 = self.conv3_bn(self.conv3(F.leaky_relu(e2, 0.2)))\n","        e4 = self.conv4_bn(self.conv4(F.leaky_relu(e3, 0.2)))\n","        e5 = self.conv5_bn(self.conv5(F.leaky_relu(e4, 0.2)))\n","        e6 = self.conv6_bn(self.conv6(F.leaky_relu(e5, 0.2)))\n","        e7 = self.conv7_bn(self.conv7(F.leaky_relu(e6, 0.2)))\n","        e8 = self.conv8(F.leaky_relu(e7, 0.2))\n","\n","        # decoding\n","        d1 = F.dropout(self.deconv1_bn(self.deconv1(F.relu(e8))), 0.5, training=True)\n","        d1 = torch.cat([d1, e7], 1)\n","        d2 = F.dropout(self.deconv2_bn(self.deconv2(F.relu(d1))), 0.5, training=True)\n","        d2 = torch.cat([d2, e6], 1)\n","        d3 = F.dropout(self.deconv3_bn(self.deconv3(F.relu(d2))), 0.5, training=True)\n","        d3 = torch.cat([d3, e5], 1)\n","        d4 = self.deconv4_bn(self.deconv4(F.relu(d3)))\n","        d4 = torch.cat([d4, e4], 1)\n","        d5 = self.deconv5_bn(self.deconv5(F.relu(d4)))\n","        d5 = torch.cat([d5, e3], 1)\n","        d6 = self.deconv6_bn(self.deconv6(F.relu(d5)))\n","        d6 = torch.cat([d6, e2], 1)\n","        d7 = self.deconv7_bn(self.deconv7(F.relu(d6)))\n","        d7 = torch.cat([d7, e1], 1)\n","        d8 = self.deconv8(F.relu(d7))\n","        out = F.tanh(d8)\n","\n","        return out\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ajYUrgp0mUt8","colab_type":"text"},"source":["#### Define Discriminator"]},{"cell_type":"code","metadata":{"id":"5upJT6CemOz6","colab_type":"code","colab":{}},"source":["class Discriminator(nn.Module):\n","    def __init__(self, d=64):\n","        # borrowed from : https://github.com/znxlwm/pytorch-pix2pix/blob/master/network.py\n","        super(Discriminator, self).__init__()\n","        self.conv1 = nn.Conv2d(6, d, 4, 2, 1)\n","        self.conv2 = nn.Conv2d(d, d * 2, 4, 2, 1)\n","        self.conv2_bn = nn.BatchNorm2d(d * 2)\n","        self.conv3 = nn.Conv2d(d * 2, d * 4, 4, 2, 1)\n","        self.conv3_bn = nn.BatchNorm2d(d * 4)\n","        self.conv4 = nn.Conv2d(d * 4, d * 8, 4, 1, 1)\n","        self.conv4_bn = nn.BatchNorm2d(d * 8)\n","        self.conv5 = nn.Conv2d(d * 8, 1, 4, 1, 1)\n","\n","    def forward(self, x):\n","        out = F.leaky_relu(self.conv1(x), 0.2)\n","        out = F.leaky_relu(self.conv2_bn(self.conv2(out)), 0.2)\n","        out = F.leaky_relu(self.conv3_bn(self.conv3(out)), 0.2)\n","        out = F.leaky_relu(self.conv4_bn(self.conv4(out)), 0.2)\n","        out = F.sigmoid(self.conv5(out))\n","        return out\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z7yQUvj2rMLU","colab_type":"text"},"source":["#### Prepare GAN model and initialize the weights"]},{"cell_type":"code","metadata":{"id":"Irabx2XtoRhY","colab_type":"code","colab":{}},"source":["#weight initialization function. \n","def weights_init(net):\n","    for m in net.modules():\n","        if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d) or isinstance(m, nn.Linear):\n","            # m.weight.data.normal_(1.0, 0.2)\n","            nn.init.xavier_normal_(m.weight.data)\n","            if m.bias is not None:\n","                m.bias.data.zero_()\n","        elif isinstance(m, nn.BatchNorm2d):\n","            m.weight.data.normal_(1.0, 0.2)\n","            m.bias.data.zero_()\n","\n","# define GAN model.\n","G = Generator().cuda()\n","D = Discriminator().cuda()\n","\n","# weight initialization \n","G.apply(weights_init)\n","D.apply(weights_init)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MhfKpgHmnQz_","colab_type":"text"},"source":["#### Start training GAN"]},{"cell_type":"code","metadata":{"id":"pJWTZUxxeQ2I","colab_type":"code","colab":{}},"source":["# install tensorboardx to use tensorboard.\n","%pip install tensorboardx\n","\n","from tensorboardX import SummaryWriter\n","from torchvision.utils import make_grid\n","import time\n","\n","# Hyper-parameters. \n","# ====== You don't need to change here ===== #\n","EPOCHS = 200\n","LAMBDA_L1 = 100\n","# ========================================== #\n","\n","# logger for tensorboard.\n","logger = SummaryWriter()\n","\n","# GT labels for calculating binary cross entropy loss. \n","real_label = torch.ones(size=(BATCH_SIZE,1)).cuda()\n","fake_label = torch.zeros(size=(BATCH_SIZE,1)).cuda()\n","\n","# criterion for binary cross entropy loss\n","BCE_criterion = torch.nn.BCELoss()\n","\n","# define optimizer. Here we use Adam optimizer. \n","optimizer_G = torch.optim.Adam(G.parameters(), lr=2e-4, betas=(0.5,0.999))\n","optimizer_D = torch.optim.Adam(D.parameters(), lr=2e-4, betas=(0.5,0.999))\n","\n","iterations = 0\n","\n","for epoch in range(EPOCHS):\n","    # Set both G&D train modes.\n","    G.train()\n","    D.train()\n","\n","    # For logging in tensorboard\n","    loss_G_total, loss_D_total = 0., 0.\n","    loss_G_sub_total, loss_D_sub_total = 0., 0.\n","\n","    for batch_idx, (real_A, real_B) in enumerate(train_loader):\n","        t1 = time.time()\n","        real_A = real_A.cuda()\n","        real_B = real_B.cuda()\n","\n","        # ============================================================ #\n","        # TODO : Fill the part for updating D&G.\n","        #  * you can generate fake_B using Generator(G).\n","        #   ex) fake_B = G(real_A)\n","        #  * first compute loss and back propgation with optimizers.\n","        #  * Note that you should compute L1 loss to when computing \n","        #    generator loss. \n","        # ============================================================ #\n","\n","        # ================= Update D ================== # \n","\n","        # Fill here. \n","        # First compute loss_D \n","        # Then update the network with loss_D using optimizer_D\n","        \n","        # ================= Update G ================== # \n","\n","        # Fill here \n","        # First compute loss_G \n","        # Then update the network with loss_G using optimizer_G.\n","        # You should also consider L1 loss between real_B, fake_B.\n","\n","\n","\n","        # For logging\n","        loss_D_total += loss_D.item()\n","        loss_G_total += loss_G.item()\n","        \n","        loss_G_sub_total += loss_G.item()\n","        loss_D_sub_total += loss_D.item()\n","\n","        # print current states\n","        print_freq = 10\n","        if batch_idx % print_freq  == 0 and batch_idx>0:\n","            print('Epoch : {} || {}/{} || loss_G={:.3f} loss_D={:.3f} time={:.3f} secs/iter'.format(\n","                epoch, batch_idx, len(train_loader), \n","                loss_G_sub_total/print_freq, loss_D_sub_total/print_freq, \n","                time.time()-t1\n","            ))\n","            # ================= Genearte example samples ================== # \n","            for bi, (real_A_test, real_B_test) in enumerate(test_loader):\n","                if bi>0:\n","                    break\n","                real_A_test = real_A_test[:NUM_EXAMPLES].cuda()\n","                real_B_test = real_B_test[:NUM_EXAMPLES].cuda()\n","                with torch.no_grad():\n","                    fake_B_test = G(real_A_test)\n","                    fake_B_test = fake_B_test[:NUM_EXAMPLES]\n","\n","                real_A_samples = (real_A_test + 1)/2\n","                real_B_samples = (real_B_test + 1)/2\n","                fake_B_samples = (fake_B_test + 1)/2\n","                examples = torch.cat((real_A_samples, fake_B_samples, real_B_samples), dim=0)\n","                examples = make_grid(examples, nrow=NUM_EXAMPLES)\n","                \n","                # log images\n","                logger.add_image('Generated_pairs', examples, iterations)\n","\n","            iterations += 1\n","            loss_G_sub_total = 0\n","            loss_D_sub_total = 0\n","\n","    loss_G_total /= len(train_loader)\n","    loss_D_total /= len(train_loader)\n","    \n","    # logging on tensorboard\n","    logger.add_scalar('loss_G', loss_G_total, epoch)\n","    logger.add_scalar('loss_D', loss_D_total, epoch)\n","\n","    # print current states\n","    print('Epoch : {} has done. AVG loss : loss_G={:.3f} loss_D={:.3f}'.format(\n","        epoch, loss_G_total, loss_D_total\n","    ))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EV838kqpQKBC","colab_type":"code","colab":{}},"source":["# Check Tensorboard.\n","%ls runs\n","%load_ext tensorboard\n","%tensorboard --logdir runs --port 9999"],"execution_count":null,"outputs":[]}]}